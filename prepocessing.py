import pandas as pd
import os
import datetime
from numpy import fft, diff, sign, abs
# from datetime import datetime
"""
In this module there are the methods for the pre-processing to be implemented before the feature extraction phase
"""
pd.options.mode.chained_assignment = None  # default='warn'


def alignment(path_muse, path_note, path_save, columns):
    """
    Merge the generated dataset with the muse device and the dataset generated by the interface.
    :param path_muse: path of muse's files
    :param path_note: path of annotation's files
    :param path_save: path where to save
    :param columns: columns to keep
    :return:
    """
    files_muse = sorted(os.listdir(path_muse))  # read muse files
    files_notes = sorted(os.listdir(path_note))  # read annotation files

    for index in range(len(files_muse)):  # for each file muse
        muse = pd.read_csv(path_muse + '/' + files_muse[index], index_col=False)  # keep the file
        print(index)

        muse = muse[['TimeStamp'] + columns]  # choose the columns
        if len(muse['TimeStamp'][0]) >= 12:  # if there is the date
            muse['TimeStamp'] = list(map(lambda x: x[11:], muse['TimeStamp']))

        note = pd.read_csv(path_note + '/' + files_notes[index], index_col=False)  #

        # fix tables
        first_check = note['check time'][1][:-3]
        note = note[2:-1]
        note['check time'] = list(map(lambda x: (x[:-3]), note['check time']))
        muse['TimeStamp'] = list(map(lambda x: str(x), muse['TimeStamp']))

        # fix dimension of csv muse
        print('Dataset size before cleaning samples:' + str(muse.shape))
        old_shape = muse.shape[0]
        muse = muse.dropna()
        muse = muse[(muse[columns] != 0).all(1)]
        print('Size of dataset after cleaning samples:' + str(muse.shape))
        print('Diference: ' + str(old_shape - muse.shape[0]))

        # muse csv must be start with the same time of note csv
        first_index = 0
        while pd.to_datetime(first_check) > pd.to_datetime(list(muse['TimeStamp'])[first_index]):
            first_index = first_index + 1
            print(first_index)
        muse = muse[first_index:]

        muse = create_new_muse_table(note, muse, first_check)

        muse.to_csv(path_save + "/" + str(index) + "alignment.csv", index=False)


def create_new_muse_table(note, muse, first_check):
    """
    The core of the union of the twp datasets
    :param note: dataframe annotation
    :param muse: dataframe muse
    :param first_check: the start time of annotation dataframe
    :return:
    """

    """
    initialization of the final dataset
    """
    note_check_time = list(note['check time'])
    muse_time_stamp = list(muse['TimeStamp'])
    user_annotation_by_note = list(note['class by user'])
    real_annotation_by_note = list(note['original class'])

    annotation = list()
    real_label = list()
    time_check = list()
    time_diff = list()
    id_image = list()

    # support variables
    keep_going = True
    i = j = 0

    while i < len(muse_time_stamp) and keep_going:
        # if time of muse is major, change answer
        if pd.to_datetime(muse_time_stamp[i]) >= pd.to_datetime(note_check_time[j]):
            j = j + 1
        # if there is an answer in note_check_time[j]
        if j < len(note_check_time):
            if j == 0:
                time_diff.append(str(pd.to_datetime(note_check_time[j]) - pd.to_datetime(first_check)))
            else:
                time_diff.append(str(pd.to_datetime(note_check_time[j]) - pd.to_datetime(note_check_time[j - 1])))
            annotation.append(user_annotation_by_note[j])
            real_label.append(real_annotation_by_note[j])
            time_check.append(pd.to_datetime(note_check_time[j]).time())
            id_image.append(j)
        # the answers are finished
        else:
            keep_going = False
        i = i + 1

    # create the new table / csv
    muse = muse[:len(annotation)]
    muse["Class By User"] = annotation
    muse["Original Class"] = real_label
    muse["Image"] = id_image
    muse["Time Check Button"] = time_check
    muse["Response Time"] = time_diff

    return muse


def labeling(path_file, path_save):
    """
    This function assigns the label attentive or distracted for each sample
    :param path_file: path to label
    :param path_save: path to save the labeled files
    :return:
    """
    files = sorted(os.listdir(path_file))
    print("Files to label:" + str(files))
    data_sets = [pd.read_csv(path_file + '/' + str(el)) for el in files]
    threshold = calculate_threshold(data_sets)  # calculate the threshold that a user is distracted or not
    print("The threshold is: " + str(threshold))
    for i in range(len(data_sets)):
        print('Datasets to label:' + str(i))
        add_label(threshold, data_sets[i])
        data_sets[i].to_csv(path_save + "/" + str(i) + "raw_f.csv", index=False)


def rc_single(df, minutes):
    """
    After ten annotate image, star to calculate the mean time response of five minutes
    :param df: dataframe with time stamp
    :param minutes: minutes for calculate the average time response
    :return: the mean time response of this dataframe
    """

    time = df['TimeStamp']
    response_time = df['Response Time']
    class_us = df['Class By User']
    change = 0
    index_start = 1

    while change <= 10:
        if response_time[index_start] != response_time[index_start - 1] and class_us[index_start] != class_us[index_start - 1]:
            change = change + 1
        index_start = index_start + 1

    time_start = pd.to_timedelta(time[index_start])
    minutes_added = datetime.timedelta(minutes=minutes)
    time_end = time_start + minutes_added

    i = index_start

    average_response_time = datetime.timedelta(hours=0, minutes=0, seconds=0)

    while pd.to_timedelta(time[i]) < time_end:
        average_response_time = average_response_time + pd.to_timedelta(response_time[i])
        i = i + 1

    return average_response_time / (i - index_start)


def add_label(threshold, df):
    """
    Distracted and attentive class assignment
    :param threshold: Response time limit for which a person is attentive
    :param df: considered dataframe
    :return: dataframe with the class added and to be saved
    """
    labels = list()
    col_indicator = list()

    annotations = [x for _, x in df.groupby(['Image'])]  # set of response

    """
    A sample is labeled as distracted if:
    - has wrong answer
    - has exceeded the threshold
    From the combination of these two possibilities the attentive and distracted classes are assigned plus an additional
    column indicating what kind of carelessness has been assigned
    """
    for answer in annotations:

        if answer["Class By User"].iloc[0] != answer["Original Class"].iloc[0] \
                and pd.to_timedelta(answer["Response Time"].iloc[0]) >= threshold:
            for i in range(len(answer)):
                labels.append('distracted')
                col_indicator.append('error-over time')
        elif answer["Class By User"].iloc[0] != answer["Original Class"].iloc[0] \
                and pd.to_timedelta(answer["Response Time"].iloc[0]) < threshold:
            for i in range(len(answer)):
                labels.append('distracted')
                col_indicator.append('error-no over time')
        elif pd.to_timedelta(answer["Response Time"].iloc[0]) >= threshold:
            for i in range(len(answer)):
                labels.append('distracted')
                col_indicator.append('over time')
        else:
            for i in range(len(answer)):
                labels.append('attentive')
                col_indicator.append('attentive')

    df['label'] = labels
    df['indicator'] = col_indicator


def calculate_threshold(data_sets):
    """
    Calculate the threshold by each datasets
    :param data_sets: the set of datasets
    :return:
    """
    sum_c = datetime.timedelta(hours=0, minutes=0, seconds=0)
    for x in [rc_single(df, 5) for df in data_sets]:  # call the function rc single, 5 is the minutes
        sum_c = sum_c + x
    return sum_c / len(data_sets)


def rolling_windows(path_file_labeled):
    """
        Calculate eye blinks rolling windows
        :param path_file_labeled: path to labeled dataset
        :return:
        """

    files_labeled = sorted(os.listdir(path_file_labeled))
    data_sets_labeled = [pd.read_csv(path_file_labeled + '/' + str(el)) for el in files_labeled]
    for i in range(len(data_sets_labeled)):
        df = data_sets_labeled[i]
        size = 7680
        step = 128
        n_examples = len(df)
        dataframes = []
        k = 0
        win_id = 0
        new_df = pd.DataFrame(columns=['window_id', 'start', 'end', 'n_blinks'])

        while k * step + size < n_examples:
            dataframes += [df.loc[k * step:k * step + size]]
            k += 1

        for el in dataframes:
            win_id += 1
            n_blink = el.Elements[el['Elements'] == '/muse/elements/blink'].count()
            start_time = el.iloc[0]['TimeStamp']
            end_time = el.iloc[-1]['TimeStamp']
            new_df = new_df.append({'window_id': win_id, 'start': start_time, 'end': end_time, 'n_blinks': n_blink},
                                   ignore_index=True)

        new_df.to_csv('muse2_file/rolling_windows/rolling_windows' + str(i) + '.csv', index=False)


def weighted_mean(rolling_window, answer):
    """
            Calculate eye blinks weighted mean
            :param rolling_window: path to rolling window csv
            :param answer: set of annotation samples
            :return:
            """

    img_start = answer['TimeStamp'].iloc[0]
    img_end = answer['TimeStamp'].iloc[-1]
    img_start = datetime.strptime(img_start, '%H:%M:%S.%f').time()
    img_end = datetime.strptime(img_end, '%H:%M:%S.%f').time()
    # rolling_window['start'] = [datetime.strptime(x, '%H:%M:%S.%f').time() for x in rolling_window['start']]
    # rolling_window['end'] = [datetime.strptime(x, '%H:%M:%S.%f').time() for x in rolling_window['end']]
    rolling_window = rolling_window[(img_start < rolling_window['end']) & (rolling_window['end'] <= img_end)]
    weights = range(1, rolling_window.shape[0] + 1)
    rolling_window['weight'] = weights
    # print("***********************************************************************************************************")
    # print(img_start)
    # print(img_end)
    # print(rolling_window)
    if len(rolling_window) > 0:
        mean = (rolling_window['n_blinks'] * rolling_window['weight']).sum() / rolling_window['weight'].sum()
        # print(mean)
        return mean
    else:
        print(img_start)
        print(img_end)
        print('There are no windows!!')


def acc_gyro_features(answer):
    """
                Features based on accelerometer and gyroscope
                :param answer: set of annotation samples
                :return: features list
                """

    acc_gyro_cols = ['Accelerometer_X', 'Accelerometer_Y', 'Accelerometer_Z', 'Gyro_X', 'Gyro_Y', 'Gyro_Z']
    # mean_cols = ['Accelerometer_Y', 'Accelerometer_Z']
    # var_cols = ['Accelerometer_X', 'Accelerometer_Y', 'Accelerometer_Z', 'Gyro_Y']
    # skew_cols = ['Gyro_Y', 'Gyro_Z']
    # max_cols = ['Accelerometer_Y', 'Gyro_X', 'Gyro_Y', 'Gyro_Z']
    # min_med_cols = ['Accelerometer_Y', 'Accelerometer_Z', 'Gyro_X', 'Gyro_Y', 'Gyro_Z']
    # std_cols = ['Accelerometer_X', 'Accelerometer_Y', 'Accelerometer_Z', 'Gyro_Y']
    peaks_list = list()
    freq_list = list()

    acc_gyro_mean = list(answer[acc_gyro_cols].mean())
    acc_gyro_var = list(answer[acc_gyro_cols].var())
    acc_gyro_skew = list(answer[acc_gyro_cols].skew())
    acc_gyro_kurt = list(answer[acc_gyro_cols].kurt())
    acc_gyro_max = answer[acc_gyro_cols].max()
    acc_gyro_max_full = answer[acc_gyro_cols].max()
    acc_gyro_max_list = list(acc_gyro_max)
    acc_gyro_min = answer[acc_gyro_cols].min()
    acc_gyro_min_full = answer[acc_gyro_cols].min()
    acc_gyro_min_list = list(acc_gyro_min)
    acc_gyro_diff = list(acc_gyro_max_full - acc_gyro_min_full)
    acc_gyro_median = list(answer[acc_gyro_cols].median())
    acc_gyro_std = list(answer[acc_gyro_cols].std())
    acc_gyro_autocorr = list(answer[acc_gyro_cols].apply(lambda x: x.autocorr()))

    for col in acc_gyro_cols:

        fourier = abs(fft.rfft(answer[col] - answer[col].mean(), n=2048))
        freq = fft.rfftfreq(2048, d=1. / 256)
        inflection = diff(sign(diff(fourier)))
        peaks = (inflection < 0).nonzero()[0] + 1

        # picchi trasformata di fourier
        peak = fourier[peaks]

        # Frequenza dei picchi della trasformata discreta di fourier
        signal_freq = freq[peaks]

        # First 5 peaks and corresponding frequencies
        mypeak = list(peak[:5])
        mysignalfreq = list(signal_freq[:5])

        if len(mypeak) < 5:
            mypeak = [0, 0, 0, 0, 0]

        if len(mysignalfreq) < 5:
            mysignalfreq = [0, 0, 0, 0, 0]

        peaks_list = peaks_list + mypeak
        freq_list = freq_list + mysignalfreq

    # step > 0.05
    # peaks_list = peaks_list[:11] + [peaks_list[14]] + peaks_list[16:20] + peaks_list[21:25] + peaks_list[27:30]

    # step > 0.1
    # peaks_list = peaks_list[7:10] + [peaks_list[19]] + [peaks_list[24]] + [peaks_list[29]]

    acc_gyro_list = acc_gyro_mean + acc_gyro_var + acc_gyro_skew + acc_gyro_kurt + acc_gyro_max_list + \
                    acc_gyro_min_list + acc_gyro_diff + acc_gyro_median + acc_gyro_std + acc_gyro_autocorr + peaks_list\
                    + freq_list

    fourier_list = peaks_list + freq_list

    return acc_gyro_list
